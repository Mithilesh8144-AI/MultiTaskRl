"""
Experience Replay Buffer for DQN

The replay buffer stores experience tuples (state, action, reward, next_state, done)
and allows random sampling to break temporal correlations in the data.

Why Replay Buffer?
==================
1. **Break Temporal Correlation**: Consecutive experiences in RL are highly correlated.
   Sampling randomly from a buffer makes the data more i.i.d. (independent and identically
   distributed), which stabilizes learning.

2. **Sample Efficiency**: Each experience can be reused multiple times for training,
   improving data efficiency.

3. **Off-Policy Learning**: DQN is an off-policy algorithm - it can learn from experiences
   generated by an older policy, not just the current one.

Implementation Details:
======================
- Uses Python lists for simplicity (could use numpy arrays or deque for optimization)
- Implements circular buffer: when full, old experiences are overwritten
- Uniform random sampling (not prioritized - that's an advanced extension)
"""

import random
import numpy as np
from collections import deque
from typing import List, Tuple, Optional


class ReplayBuffer:
    """
    Fixed-size buffer to store experience tuples for off-policy learning.

    Attributes:
        capacity (int): Maximum number of experiences to store
        buffer (deque): Double-ended queue that automatically removes oldest items when full
        position (int): Current write position (for circular buffer behavior)
    """

    def __init__(self, capacity: int = 100000):
        """
        Initialize the replay buffer.

        Args:
            capacity (int): Maximum number of transitions to store. When the buffer
                          is full, oldest experiences are discarded. Default: 100k
        """
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)  # Automatically handles overflow
        self.position = 0

    def push(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        done: bool
    ) -> None:
        """
        Add a new experience tuple to the buffer.

        Args:
            state (np.ndarray): Current state observation
            action (int): Action taken (discrete action index)
            reward (float): Reward received
            next_state (np.ndarray): Next state observation
            done (bool): Whether the episode terminated
        """
        # Store as tuple: (s, a, r, s', done)
        experience = (state, action, reward, next_state, done)
        self.buffer.append(experience)

    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:
        """
        Randomly sample a batch of experiences from the buffer.

        Args:
            batch_size (int): Number of experiences to sample

        Returns:
            Tuple of numpy arrays: (states, actions, rewards, next_states, dones)
                - states: (batch_size, state_dim)
                - actions: (batch_size,)
                - rewards: (batch_size,)
                - next_states: (batch_size, state_dim)
                - dones: (batch_size,)

        Raises:
            ValueError: If batch_size > len(buffer)
        """
        if batch_size > len(self.buffer):
            raise ValueError(
                f"Cannot sample {batch_size} experiences from buffer of size {len(self.buffer)}"
            )

        # Random sample without replacement
        batch = random.sample(self.buffer, batch_size)

        # Unzip the batch into separate arrays
        # batch is a list of tuples: [(s1,a1,r1,s1',d1), (s2,a2,r2,s2',d2), ...]
        # zip(*batch) transposes it to: [(s1,s2,...), (a1,a2,...), ...]
        states, actions, rewards, next_states, dones = zip(*batch)

        # Convert to numpy arrays for efficient computation
        # States are already numpy arrays, just stack them
        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards, dtype=np.float32)
        next_states = np.array(next_states)
        dones = np.array(dones, dtype=np.float32)  # Convert bool to float for computation

        return states, actions, rewards, next_states, dones

    def __len__(self) -> int:
        """Return the current size of the buffer."""
        return len(self.buffer)

    def is_ready(self, batch_size: int) -> bool:
        """
        Check if buffer has enough experiences to sample a batch.

        Args:
            batch_size (int): Desired batch size

        Returns:
            bool: True if buffer size >= batch_size
        """
        return len(self.buffer) >= batch_size

    def clear(self) -> None:
        """Clear all experiences from the buffer."""
        self.buffer.clear()
        self.position = 0

    def __repr__(self) -> str:
        """String representation of the buffer."""
        return f"ReplayBuffer(capacity={self.capacity}, size={len(self.buffer)})"


# Example usage and testing
if __name__ == "__main__":
    print("Testing Replay Buffer")
    print("=" * 60)

    # Create a small buffer for testing
    buffer = ReplayBuffer(capacity=1000)
    print(f"Created: {buffer}\n")

    # Simulate adding experiences
    print("Adding 10 random experiences...")
    for i in range(10):
        state = np.random.randn(8)  # Lunar Lander state dim = 8
        action = np.random.randint(0, 4)  # 4 discrete actions
        reward = np.random.randn()
        next_state = np.random.randn(8)
        done = (i == 9)  # Last one is terminal

        buffer.push(state, action, reward, next_state, done)

    print(f"Buffer size: {len(buffer)}\n")

    # Test sampling
    print("Sampling a batch of 5 experiences...")
    if buffer.is_ready(5):
        states, actions, rewards, next_states, dones = buffer.sample(5)
        print(f"  States shape: {states.shape}")
        print(f"  Actions shape: {actions.shape}")
        print(f"  Rewards shape: {rewards.shape}")
        print(f"  Next states shape: {next_states.shape}")
        print(f"  Dones shape: {dones.shape}")
        print(f"\n  Sample actions: {actions}")
        print(f"  Sample rewards: {rewards}")
        print(f"  Sample dones: {dones}")
    else:
        print("  Not enough experiences in buffer!")

    print("\n" + "=" * 60)
    print("Replay buffer test complete!")
