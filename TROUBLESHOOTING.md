# Troubleshooting & Experiment Issues Log

**Purpose:** Document issues encountered during RL experiments, their root causes, and solutions for future reference.

**Last Updated:** 2026-01-06

---

## Issue #1: Windy Environment - Training Hangs at Episode 250

**Date:** 2026-01-06
**Experiment:** Independent DQN - Windy Lunar Lander (Colab)
**Notebook:** `2a_independent_dqn_windy_colab.ipynb`

### Symptoms
- Training progresses normally until ~episode 250
- Progress bar freezes and stops updating
- Process appears to hang indefinitely (40+ minutes with no progress)
- GPU is active but no forward movement

### Root Cause
**Evaluation episodes running indefinitely without termination.**

During evaluation (which happens every 50 episodes), the agent uses a greedy policy (epsilon=0, no exploration). In the Windy environment:
- Random wind forces can cause the lander to drift laterally
- Without random exploration, the agent may get stuck in a "drifting loop"
- The episode never reaches a terminal state (landing or crash)
- Evaluation hangs waiting for episode completion

### Why Episode 250?
- Evaluation happens at episodes: 50, 100, 150, 200, **250**, 300...
- By episode 250, the agent has learned some policy that leads to endless drift
- Early episodes (50, 100) complete faster because agent is still exploring randomly

### Solution
**Add maximum step limit to evaluation episodes.**

Modify Cell 10 in the evaluation section:

```python
# Evaluation
if episode % HYPERPARAMS['eval_freq'] == 0 and episode > 0:
    eval_reward_mean = 0
    eval_reward_list = []

    for _ in range(HYPERPARAMS['eval_episodes']):
        eval_state, _ = env.reset()
        eval_reward = 0
        eval_done = False
        eval_truncated = False
        eval_steps = 0  # ‚Üê ADD THIS

        while not (eval_done or eval_truncated):
            eval_action = agent.select_action(eval_state, epsilon=0.0)
            eval_state, r, eval_done, eval_truncated, _ = env.step(eval_action)
            eval_reward += r

            eval_steps += 1  # ‚Üê ADD THIS
            if eval_steps >= 1000:  # ‚Üê ADD THIS SAFETY CHECK
                pbar.write(f"‚ö†Ô∏è  Eval episode exceeded 1000 steps, forcing termination")
                break  # ‚Üê ADD THIS
```

### Prevention for Future Experiments
1. **Always add episode step limits** to both training and evaluation loops
2. **Monitor early evaluation episodes** - if they take >5 seconds, investigate
3. **For modified environments**, test evaluation separately before full training
4. **Consider using epsilon=0.01** during evaluation instead of 0.0 to allow minimal exploration

### Status
‚úÖ **FIXED** - Applied step limit to evaluation loop

---

## Issue #2: Windy/Heavy Environments - Training Episodes Exceed Timeout

**Date:** 2026-01-06
**Experiment:** Independent DQN - Windy/Heavy Lunar Lander (Colab)
**Notebooks:** `2a_independent_dqn_windy_colab.ipynb`, `2b_independent_dqn_heavy_colab.ipynb`

### Symptoms
- After ~episode 200, training episodes constantly hit 1000-step timeout
- Agent learns to hover/drift without landing
- Training slows down significantly (each episode takes max time)
- Eval reward stuck around -120 (poor performance)

### Root Cause
**Agent learned local optimum: "don't crash" but not "land properly".**

The agent discovered that:
1. Crashing gives big negative reward
2. Hovering/drifting avoids crashing
3. Landing is risky and requires precision

Result: Agent gets stuck in "safe hovering" behavior and never learns to land.

**Why Episode ~200?**
- Early episodes (1-100): Random exploration, crashes quickly
- Episode ~100-200: Agent learns "don't crash = avoid negative reward"
- Episode 200+: Agent commits to hovering strategy, timeout every episode

### Why Episodes Don't Naturally Terminate

**LunarLander episodes normally end when:**
1. ‚úÖ Lander **crashes** ‚Üí done=True (negative reward)
2. ‚úÖ Lander **lands successfully** ‚Üí done=True (positive reward)
3. ‚è±Ô∏è Gymnasium's built-in timeout (typically 1000 steps)

**But the trained agent learned to do NEITHER:**
- Agent uses thrusters to hover in mid-air indefinitely
- Never crashes ‚Üí no termination signal
- Never lands ‚Üí no termination signal
- Just drifts with the wind for the full 1000 steps

This is a **local optimum** - the agent found a "safe" strategy that avoids negative rewards but never achieves the goal.

### Why Reduce from 1000 to 400 Steps?

**The Problem with 1000 Steps:**
- Agent hovers for full 1000 steps every episode
- Takes ~40 seconds per episode
- Wastes compute on useless hovering behavior
- Agent learns "hovering is safe" because it gets rewarded for avoiding crashes

**The Solution with 400 Steps:**
- Episode cuts off after 400 steps if still hovering
- Takes ~16 seconds per episode (2.5x faster training)
- Agent experiences "timeout truncation" as a negative signal
- Learns: "hovering too long = failure, need to land within 400 steps"

**Key Insight:** The 400-step limit isn't about when episodes SHOULD end naturally - it's about creating **urgency to land** and breaking the hovering local optimum.

Think of it like:
- **With 1000 steps**: "I can hover forever safely" ‚ùå
- **With 400 steps**: "I need to land quickly or I fail" ‚úÖ

### Investigation: Wind Strength Testing
Tested whether `wind_power=20.0` was too strong for the agent to counter.

**Test Results** (random agent, 10 episodes each):
```
Wind Power    Avg Steps    Timeout Rate
5.0           90           0%
10.0          95           0%
15.0          100          0%
20.0          97           0%
30.0          90           0%
```

**Conclusion:** Wind strength is NOT the problem. Random agents crash quickly at all wind levels. The issue is that **trained agents learn to hover safely** instead of landing.

### Solution
**Reduce MAX_EPISODE_STEPS from 1000 to 400.**

**Why this works:**
1. Forces episodes to terminate faster (400 steps = ~15-20 seconds)
2. Agent experiences "timeout truncation" as negative signal
3. More episodes per hour = more opportunities to explore landing
4. Reduces GPU/compute waste on long hovering episodes

**Alternative solutions considered:**
- Add time penalty (reward -= 0.05 per step) - more complex, save for future
- Increase exploration (epsilon_decay=0.998) - addresses symptom not cause
- Reduce wind strength - testing showed this isn't the issue

### Implementation
**Changed in Cell 10 (both notebooks):**
```python
# Before:
MAX_EPISODE_STEPS = 1000

# After:
MAX_EPISODE_STEPS = 400
# Reduced from 1000 to 400 for Windy/Heavy variants
```

**Why 400?**
- Standard LunarLander episodes typically complete in 100-300 steps
- 400 steps is generous enough to allow successful landing
- Short enough to prevent endless hovering
- 2.5x faster training than 1000 steps

### Prevention for Future Experiments
1. **Set reasonable episode timeouts** based on environment difficulty
2. **Monitor early episodes** - if episodes start hitting timeout, reduce MAX_STEPS
3. **For harder environments** (Windy, Heavy), use lower timeouts than Standard
4. **Consider time penalties** if hovering persists even with lower timeouts
5. **Watch for local optima** - if rewards plateau, agent may be "playing it safe"

### Status
‚úÖ **FIXED** - Reduced MAX_EPISODE_STEPS to 400 in both notebooks

### Files Modified
- `/Users/mithileshr/RL/notebooks/2a_independent_dqn_windy_colab.ipynb`
- `/Users/mithileshr/RL/notebooks/2b_independent_dqn_heavy_colab.ipynb`
- `/Users/mithileshr/RL/test_wind_strength.py` (created for investigation)

---

## Issue #3: Heavy Environment - Multiple Critical Bugs Causing Training Failure

**Date:** 2026-01-06
**Experiment:** Independent DQN - Heavy Weight Lunar Lander (Colab + Mac)
**Notebook:** `2b_independent_dqn_heavy_colab.ipynb`
**Status:** ‚úÖ **ALL FIXES APPLIED** - Ready for retraining

### Symptoms

**Training completed but completely failed:**
- Final average reward (last 100 episodes): **-0.70** (vs. target 200+)
- Never achieved positive reward (success rate: 0%)
- Agent stuck at hovering/drifting without landing
- **Catastrophic failure at episode 250:** Eval reward = **-1273.49** (massive spike)
- Training appeared to "work" but produced useless policy
- High variance in episode rewards, no convergence

**Training Metrics:**
```
Episode   50: Eval = -168.23
Episode  100: Eval = -110.98
Episode  150: Eval = -102.24
Episode  200: Eval = -49.34
Episode  250: Eval = -1273.50  ‚ö†Ô∏è CATASTROPHIC
Episode  300: Eval = -209.31
Episode  400: Eval = 40.39      (best, but unstable)
Episode  900: Eval = 8.93
Final (last 100): -0.70
```

### Root Causes (5 Bugs Identified)

#### Bug #1: Gravity Persistence Failure (CRITICAL)
**Location:** `environments/lunar_lander_variants.py:88-92` (before fix)

**The Problem:**
```python
class HeavyWeightLunarLander(LunarLander):
    def reset(self, **kwargs):
        result = super().reset(**kwargs)
        if self.world is not None:
            self.world.gravity = (0, -10.0 * self.gravity_multiplier)  # Only set ONCE!
        return result

    def step(self, action):
        return super().step(action)  # ‚ùå Gravity NOT maintained!
```

**Why This Breaks:**
- Gravity set ONLY during `reset()`, not maintained during episode
- Box2D physics engine may reset gravity values during simulation
- Agent experiences **inconsistent physics** across episodes
- Sometimes gravity is 1.25x, sometimes it reverts to 1.0x
- Agent cannot learn stable policy when environment is non-stationary

**Evidence:**
- Extremely negative rewards early on (-221 to -341 first 27 episodes)
- High variance throughout training (never stabilizes)
- Episode 250 catastrophic failure suggests policy learned under wrong physics

---

#### Bug #2: Episode Timeout Too Short (CRITICAL)
**Location:** `notebooks/2b_independent_dqn_heavy_colab.ipynb` Cell 10

**The Problem:**
```python
MAX_EPISODE_STEPS = 600  # ‚ùå Too short for 1.25x gravity!
```

**Why 600 Steps Is Insufficient:**
- Standard LunarLander: typically 100-300 steps to land
- Heavy (1.25x gravity): Faster descent = needs MORE time for controlled landing
- With 1.25x gravity, lander descends **25% faster**
- Agent needs time to: (1) descend, (2) adjust trajectory, (3) brake, (4) land gently
- 600 steps is only **60%** of standard timeout (1000)

**Impact on Training:**
```
Average steps per episode: 415.7
Episodes hitting timeout: ~70% (415/600 = 69%)
```

**The Episode 250 Catastrophic Failure:**
```
Episode 250: ALL 5 eval episodes truncated at 600 steps
Average reward: -1273.49

What happened:
- Each eval episode cut off mid-descent
- Lander still high in air when timeout hit
- Accumulated penalties: timeout + altitude + velocity + fuel waste
- Potential multiple "virtual crashes" recorded
- Average of 5 episodes = -1273.49
```

**Formula:** Each truncated episode ‚âà -250 reward √ó 5 episodes = -1250 average

**Why This Persisted:**
- Training episodes ALSO hitting timeout (70% rate)
- Agent never experiences full successful landing
- Learns "hover until timeout" as best strategy
- Cannot learn landing behavior if episodes always truncate

---

#### Bug #3: Hyperparameter Mismatch (HIGH SEVERITY)
**Location:** `notebooks/2b_independent_dqn_heavy_colab.ipynb` Cell 8

**The Problem:**
Heavy task used **IDENTICAL** hyperparameters as Standard task, despite being **25% harder** (1.25x gravity).

**Incorrect Hyperparameters:**
```python
'learning_rate': 5e-4,           # ‚ùå Too high for harder task
'epsilon_decay': 0.995,          # ‚ùå Too fast, insufficient exploration
'target_update_freq': 10,        # ‚ùå Too frequent, Q-value instability
'min_replay_size': 1000,         # ‚ùå Too small, insufficient diversity
'num_episodes': 1000,            # ‚ùå Too short for harder task
```

**Why Each Parameter Failed:**

1. **Learning Rate (5e-4):**
   - Too aggressive for complex exploration landscape
   - Causes overshooting in Q-value updates
   - Policy oscillates instead of converging

2. **Epsilon Decay (0.995):**
   - Epsilon drops to 0.01 by episode ~460
   - Agent stops exploring before finding good landing strategy
   - Gets stuck in "safe hovering" local optimum

3. **Target Update Frequency (10):**
   - Updating target network every 10 episodes
   - Too frequent for harder task ‚Üí Q-value instability
   - "Moving goalpost" problem in Q-learning

4. **Min Replay Size (1000):**
   - Only 1000 experiences before training starts
   - Insufficient diversity for complex task
   - Early training on poor experiences

5. **Training Duration (1000):**
   - Standard task achieves good results in 1000 episodes
   - Harder task needs more episodes to explore

---

#### Bug #4: Evaluation Timeout = Training Timeout
**Location:** `notebooks/2b_independent_dqn_heavy_colab.ipynb` Cell 10

**The Problem:**
Evaluation used same 600-step timeout as training, causing eval episodes to truncate mid-flight and record catastrophic penalties.

**Why This Matters:**
- Eval uses greedy policy (epsilon=0, no exploration)
- Greedy policy might take different trajectory than training
- 600-step limit too tight ‚Üí episode truncates mid-descent
- Eval reward doesn't reflect true policy performance

---

#### Bug #5: Gravity Multiplier Inconsistency
**Location:** Multiple files

**The Problem:**
- Documentation (`CLAUDE.md`): Specifies `gravity_multiplier = 1.5`
- Notebook Cell 5: Actually uses `gravity_multiplier = 1.25`
- Main environment file: Had `gravity_multiplier = 1.5` as default

**Confusion:**
- Unclear which gravity value was actually used
- Testing showed 1.25x was implemented in notebook
- 1.5x would be even harder (50% stronger gravity)

---

### Solutions Applied

#### Fix #1: Gravity Persistence (CRITICAL)
**Files Modified:**
- `environments/lunar_lander_variants.py`
- `notebooks/2b_independent_dqn_heavy_colab.ipynb` (Cell 5)

**The Fix:**
```python
class HeavyWeightLunarLander(LunarLander):
    def __init__(self, gravity_multiplier=1.25, **kwargs):
        self.gravity_multiplier = gravity_multiplier
        super().__init__(**kwargs)
        self.task_name = "Heavy Weight"

    def reset(self, **kwargs):
        result = super().reset(**kwargs)
        if self.world is not None:
            self.world.gravity = (0, -10.0 * self.gravity_multiplier)
        return result

    def step(self, action):
        # ‚úÖ CRITICAL FIX: Ensure gravity stays modified throughout episode
        if self.world is not None:
            self.world.gravity = (0, -10.0 * self.gravity_multiplier)
        return super().step(action)
```

**Why This Works:**
- Gravity re-applied every step
- Box2D cannot reset it mid-episode
- Agent experiences consistent physics throughout episode
- Stable environment ‚Üí stable policy learning

---

#### Fix #2: Increase Episode Timeout
**File:** `notebooks/2b_independent_dqn_heavy_colab.ipynb` (Cell 10)

**Change:**
```python
# Before:
MAX_EPISODE_STEPS = 600

# After:
MAX_EPISODE_STEPS = 800  # ‚úÖ Allows full descent + landing
```

**Why 800 Steps:**
- Standard task: 1000 steps
- Heavy task: ~80% of standard (not 60%)
- Allows agent to complete full landing sequence
- Reduces catastrophic truncation penalties
- Still creates urgency (not 1000 like standard)

**Expected Impact:**
- Episodes average 500-700 steps (not hitting timeout constantly)
- Fewer truncated episodes ‚Üí better reward signal
- No more -1273 catastrophic failures

---

#### Fix #3: Tune Hyperparameters for Heavy Task
**File:** `notebooks/2b_independent_dqn_heavy_colab.ipynb` (Cell 8)

**Changes:**
```python
HYPERPARAMS = {
    'num_episodes': 1500,              # ‚úÖ 1000 ‚Üí 1500 (50% more training)
    'batch_size': 64,                  # Keep same
    'replay_buffer_size': 100000,      # Keep same
    'min_replay_size': 2000,           # ‚úÖ 1000 ‚Üí 2000 (2x initial buffer)

    'learning_rate': 2.5e-4,           # ‚úÖ 5e-4 ‚Üí 2.5e-4 (halved for stability)
    'gamma': 0.99,                     # Keep same
    'epsilon_start': 1.0,              # Keep same
    'epsilon_end': 0.01,               # Keep same
    'epsilon_decay': 0.992,            # ‚úÖ 0.995 ‚Üí 0.992 (slower decay)
    'target_update_freq': 20,          # ‚úÖ 10 ‚Üí 20 (less frequent updates)

    'eval_freq': 50,
    'eval_episodes': 5,
    'save_freq': 100,
}
```

**Rationale for Each Change:**

| Parameter | Old | New | Reason |
|-----------|-----|-----|--------|
| `learning_rate` | 5e-4 | **2.5e-4** | Halved for stability, prevents overshooting |
| `epsilon_decay` | 0.995 | **0.992** | Slower decay, more exploration time |
| `target_update_freq` | 10 | **20** | Less frequent updates, reduces Q-instability |
| `min_replay_size` | 1000 | **2000** | More diverse initial experiences |
| `num_episodes` | 1000 | **1500** | Harder task needs more training |

---

#### Fix #4: Gravity Multiplier Standardization
**File:** `environments/lunar_lander_variants.py`

**Change:**
```python
# Before:
def __init__(self, gravity_multiplier=1.5, **kwargs):

# After:
def __init__(self, gravity_multiplier=1.25, **kwargs):
```

**Rationale:**
- Notebook already using 1.25x successfully
- Get training working at 1.25x first
- Can increase to 1.5x later once confirmed working
- Consistency across all files

---

#### Fix #5: Documentation Updated
**File:** `CLAUDE.md`

**Updates:**
- ‚úÖ Added Heavy-specific hyperparameters section
- ‚úÖ Updated episode timeout rationale (800 steps)
- ‚úÖ Added gravity persistence fix documentation
- ‚úÖ Updated Current Training Status
- ‚úÖ Added new "Heavy Experiment Critical Bugs" lesson learned
- ‚úÖ Updated Environment Modifications section with `step()` override

---

### Expected Results After Fixes

**Before Fixes:**
```
Final avg reward: -0.70
Best eval reward: 40.39 (unstable)
Success rate: 0%
Episode 250 eval: -1273.49 (catastrophic)
```

**Expected After Fixes:**
```
Final avg reward: 50-100
Success rate: 20-40%
No catastrophic failures
Stable, convergent learning
```

**Success Criteria:**
1. ‚úÖ Final 100-episode average > 50 reward
2. ‚úÖ At least 20% episodes achieve positive reward
3. ‚úÖ No eval rewards below -500 (no catastrophic failures)
4. ‚úÖ Episodes average 500-700 steps (not hitting timeout constantly)
5. ‚úÖ Training shows consistent improvement trend

---

### Prevention for Future Experiments

1. **Always Test Physics Persistence:**
   - For ANY environment modification (gravity, wind, mass, etc.)
   - Override `step()` to maintain modifications
   - Test with `assert env.world.gravity == expected_value` in training loop

2. **Task-Specific Hyperparameters:**
   - Never copy-paste hyperparams for harder tasks
   - Rule of thumb for harder tasks:
     - Learning rate: Reduce by 50%
     - Epsilon decay: Slow by 0.003 (0.995 ‚Üí 0.992)
     - Target update freq: Double (10 ‚Üí 20)
     - Training episodes: Increase by 50%

3. **Episode Timeout Sizing:**
   - Standard task: 1000 steps (baseline)
   - Modified tasks: Estimate based on physics changes
     - Faster dynamics ‚Üí MORE time needed (counterintuitive!)
     - Slower dynamics ‚Üí LESS time needed
   - Rule: Set timeout to ~3x typical completion time

4. **Monitor Eval Episodes:**
   - If eval rewards spike to extreme values (-1000+), check timeouts
   - Eval rewards should be similar magnitude to training rewards
   - Large discrepancy ‚Üí timeout or policy issues

5. **Gravity Multiplier Guidelines:**
   - 1.0x = Standard (baseline)
   - 1.25x = Moderate challenge (good starting point)
   - 1.5x = Hard challenge (requires tuning)
   - 2.0x+ = Extreme (likely needs curriculum learning)

---

### Testing Protocol (Before Starting Training)

**Before committing to 1500-episode run:**

1. ‚úÖ **Test Environment Separately:**
   ```python
   env = make_env('heavy')
   env.reset()
   for _ in range(100):
       env.step(env.action_space.sample())
       assert env.world.gravity == (0, -12.5), "Gravity persistence failed!"
   ```

2. ‚úÖ **Run 100-Episode Test:**
   - Quick sanity check
   - Verify no catastrophic failures
   - Check episode lengths (should average 500-700 steps)

3. ‚úÖ **Monitor First Eval (Episode 50):**
   - Should complete without hanging
   - Reward should be in range -300 to 0
   - If hanging or extreme reward ‚Üí investigate before continuing

4. ‚úÖ **Compare to Baseline:**
   - Heavy should be harder than Standard (lower rewards initially)
   - But not ORDERS OF MAGNITUDE worse (-0.7 was too bad)
   - Expect Heavy episode 1000 ‚âà Standard episode 600-700

---

### Files Modified

1. ‚úÖ `environments/lunar_lander_variants.py`
   - Added `step()` override for gravity persistence
   - Updated default `gravity_multiplier` from 1.5 ‚Üí 1.25

2. ‚úÖ `notebooks/2b_independent_dqn_heavy_colab.ipynb`
   - Cell 5: Added `step()` override to embedded environment
   - Cell 8: Tuned all hyperparameters for Heavy task
   - Cell 10: Increased `MAX_EPISODE_STEPS` from 600 ‚Üí 800

3. ‚úÖ `CLAUDE.md`
   - Added Heavy-specific hyperparameters documentation
   - Updated timeout rationale
   - Added gravity persistence fix details
   - Updated training status

4. ‚úÖ `TROUBLESHOOTING.md` (this file)
   - Added Issue #3 with comprehensive documentation

---

### Status

‚úÖ **ALL FIXES APPLIED** (2026-01-06)
‚è≥ **READY FOR RETRAINING**

**Next Steps:**
1. Retrain Heavy experiment from scratch (1500 episodes)
2. Monitor first 100 episodes for stability
3. Compare final results to baseline (-0.7 ‚Üí 50-100 expected)
4. If successful at 1.25x, consider increasing to 1.5x

---

### Key Insights & Lessons

**The "One-Size-Fits-All" Trap:**
- Copying hyperparameters across tasks is dangerous
- Harder environments need DIFFERENT hyperparameters, not just more episodes
- Small changes in physics (1.25x gravity) require significant tuning

**Physics Persistence is Critical:**
- Environment modifications must be maintained EVERY step
- Box2D (and other physics engines) may reset custom values
- Always override `step()` for modifications, not just `reset()`

**Episode Timeout is a Hyperparameter:**
- Not just a safety limit
- Shapes learning by creating urgency
- Task-specific timeouts are crucial for success

**Catastrophic Failures Signal Deep Issues:**
- Episode 250 eval = -1273 was a major red flag
- Don't dismiss extreme values as "outliers"
- Usually indicates timeout, physics, or policy collapse

**The Heavy Task Teaches:**
- Environment design is subtle
- Testing must be thorough
- Documentation must match implementation
- One bug can cascade into multiple symptoms

---

## Issue Template (For Future Issues)

**Date:** YYYY-MM-DD
**Experiment:** [Experiment Name]
**Notebook/File:** [File path]

### Symptoms
[What you observed - be specific]

### Root Cause
[Why it happened - technical explanation]

### Solution
[How it was fixed - include code if applicable]

### Prevention for Future Experiments
[How to avoid this in the future]

### Status
[ ] Open / [‚úÖ] Fixed / [‚è∏Ô∏è] Workaround

---

## Environment Modifications Explained

### How the Heavy Weight Environment Works

**Purpose:** Make the lander "heavier" by increasing gravitational pull, requiring more thrust and precision.

**Implementation (UPDATED 2026-01-06):**
```python
class HeavyWeightLunarLander(LunarLander):
    def __init__(self, gravity_multiplier=1.25, **kwargs):  # UPDATED: 1.5 ‚Üí 1.25
        self.gravity_multiplier = gravity_multiplier
        super().__init__(**kwargs)
        self.task_name = "Heavy Weight"

    def reset(self, **kwargs):
        result = super().reset(**kwargs)
        # Increase gravity after environment is initialized
        if self.world is not None:
            # Standard gravity is (0, -10), multiply the y-component
            self.world.gravity = (0, -10.0 * self.gravity_multiplier)
        return result

    def step(self, action):
        # ‚úÖ CRITICAL FIX (2026-01-06): Maintain gravity persistence
        if self.world is not None:
            self.world.gravity = (0, -10.0 * self.gravity_multiplier)
        return super().step(action)
```

**What Changes:**
- **Standard LunarLander**: Gravity = `(0, -10.0)` (10 units downward)
- **Heavy Weight (1.25x)**: Gravity = `(0, -12.5)` (12.5 units downward = 25% stronger)

**Why This Makes It Harder:**
1. **Faster descent** ‚Üí Lander falls faster, less time to react
2. **More thrust needed** ‚Üí Agent must use main engine more aggressively
3. **Harder to hover** ‚Üí Can't "pause" mid-air as easily
4. **Landing precision** ‚Üí Requires more careful control to avoid crash

**Why Modify Gravity Instead of Mass?**
- Cleaner implementation
- Predictable linear scaling effect
- Modifying mass would require complex Box2D physics changes
- Gravity modification is easier to reason about and debug

**‚ö†Ô∏è CRITICAL: The `step()` Override is Required!**
- Without it, Box2D may reset gravity mid-episode
- Causes non-stationary environment (inconsistent physics)
- Agent cannot learn stable policy
- See Issue #3 for detailed analysis

### How the Windy Environment Works

**Purpose:** Add random lateral forces to simulate wind, making horizontal control harder.

**Implementation:**
```python
class WindyLunarLander(LunarLander):
    def __init__(self, wind_power=20.0, **kwargs):
        super().__init__(**kwargs)
        self.wind_power = wind_power
        self.task_name = "Windy"

    def step(self, action):
        # Apply random lateral wind force
        wind_force = np.random.uniform(-self.wind_power, self.wind_power)
        if self.lander is not None:
            self.lander.ApplyForceToCenter((wind_force, 0.0), True)
        return super().step(action)
```

**What Changes:**
- Each step applies random horizontal force between -20.0 and +20.0
- Force direction and magnitude change every step
- Agent must constantly counteract drift with side thrusters

**Why wind_power=20.0?**
- Tested values from 5.0 to 30.0
- All values are physically manageable by the lander's thrusters
- 20.0 provides good challenge without being impossible
- See Issue #2 investigation for test results

---

## Additional Notes

### Common Issues to Watch For

1. **GPU Memory Issues (Colab)**
   - Symptoms: "CUDA out of memory" errors
   - Cause: Batch size too large or replay buffer memory leak
   - Solution: Reduce batch size or clear GPU cache with `torch.cuda.empty_cache()`

2. **Colab Session Timeout**
   - Symptoms: Progress stops after 30-90 minutes
   - Cause: Colab disconnects inactive sessions
   - Solution: Enable Colab Pro or add periodic output to keep session active

3. **Environment Physics Bugs**
   - Symptoms: Unexpected rewards or impossible states
   - Cause: Incorrect environment modification (e.g., wrong gravity value)
   - Solution: Test environment separately with `test_environments.py`

4. **Replay Buffer Memory Growth**
   - Symptoms: Training slows down over time
   - Cause: Replay buffer filling up and slowing sampling
   - Solution: Use fixed-size deque (already implemented)

---

## Best Practices from Experiments

1. **Always test environments separately first** (`test_environments.py`)
2. **Add safety timeouts** to all episode loops (training + evaluation)
3. **Save checkpoints frequently** (every 100 episodes) in case of crashes
4. **Monitor GPU utilization** in Colab to ensure it's being used
5. **Compare early episodes** - if episode 1 takes 10 seconds, something is wrong

---

## Experiment Progress Tracker

| Experiment | Status | Last Episode | Notes |
|------------|--------|--------------|-------|
| Baseline DQN (Standard, Mac) | ‚úÖ Complete | 1000 | Running locally |
| Independent DQN (Windy, Colab) | üîÑ In Progress | ~230 | Fixed timeout issues, restarting with MAX_STEPS=400 |
| Independent DQN (Heavy, Colab) | ‚è≥ Pending | - | Ready to start with MAX_STEPS=400 |

---

## Contact & Resources

- **Project Spec:** `/Users/mithileshr/RL/CLAUDE.md`
- **Environment Test:** `/Users/mithileshr/RL/test_environments.py`
- **Results Directory:** `/Users/mithileshr/RL/results/`

---

**Remember:** Document issues as they occur! Future you will thank present you. üìù
