{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Independent DQN - Heavy Weight Lunar Lander (Colab)\n\n**Task:** Heavy Weight Lunar Lander (gravity_multiplier=1.5)\n\n**Method:** Independent DQN (Phase 3 - Multi-Task Baseline)\n\nThis notebook is self-contained and designed to run on Google Colab with GPU acceleration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q gymnasium[box2d]\n",
    "!apt-get install -y swig > /dev/null 2>&1\n",
    "!pip install -q box2d-py\n",
    "\n",
    "print(\"‚úì Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & GPU Configuration\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import Tuple, Dict, List\n",
    "import gymnasium as gym\n",
    "\n",
    "# GPU Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚úì GPU acceleration enabled!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: ReplayBuffer Class (Embedded)\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialize replay buffer with given capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        states = np.array([t[0] for t in batch])\n",
    "        actions = np.array([t[1] for t in batch])\n",
    "        rewards = np.array([t[2] for t in batch])\n",
    "        next_states = np.array([t[3] for t in batch])\n",
    "        dones = np.array([t[4] for t in batch])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return current size of buffer.\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ReplayBuffer(capacity={self.capacity}, size={len(self.buffer)})\"\n",
    "\n",
    "print(\"‚úì ReplayBuffer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: QNetwork & DQNAgent Classes (Embedded)\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for DQN.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        \"\"\"Initialize Q-Network.\n",
    "\n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with epsilon-greedy exploration.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        learning_rate: float = 5e-4,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.01,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        target_update_freq: int = 10,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        \"\"\"Initialize DQN Agent.\"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.device = device\n",
    "\n",
    "        # Networks\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Training stats\n",
    "        self.episodes = 0\n",
    "\n",
    "    def select_action(self, state, epsilon=None):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def update(self, batch):\n",
    "        \"\"\"Update Q-network using a batch of transitions.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Current Q values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'episodes': self.episodes\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.episodes = checkpoint['episodes']\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DQNAgent(state_dim={self.state_dim}, action_dim={self.action_dim}, epsilon={self.epsilon:.4f}, steps={self.episodes})\"\n",
    "\n",
    "print(\"‚úì QNetwork and DQNAgent classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Environment Variants (Embedded)\n",
    "from gymnasium.envs.box2d.lunar_lander import LunarLander\n",
    "\n",
    "class StandardLunarLander(LunarLander):\n",
    "    \"\"\"Standard Lunar Lander environment (unchanged).\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.task_name = \"Standard\"\n",
    "\n",
    "\n",
    "class WindyLunarLander(LunarLander):\n",
    "    \"\"\"Windy variant with random lateral wind forces.\"\"\"\n",
    "\n",
    "    def __init__(self, wind_power=20.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wind_power = wind_power\n",
    "        self.task_name = \"Windy\"\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Step with wind force applied.\"\"\"\n",
    "        # Apply random lateral wind\n",
    "        wind_force = np.random.uniform(-self.wind_power, self.wind_power)\n",
    "        if self.lander is not None:\n",
    "            self.lander.ApplyForceToCenter((wind_force, 0.0), True)\n",
    "\n",
    "        return super().step(action)\n",
    "\n",
    "\n",
    "class HeavyWeightLunarLander(LunarLander):\n",
    "    \"\"\"Heavy weight variant with increased gravity.\"\"\"\n",
    "\n",
    "    def __init__(self, gravity_multiplier=1.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gravity_multiplier = gravity_multiplier\n",
    "        self.task_name = \"Heavy Weight\"\n",
    "        # Increase gravity\n",
    "        self.world.gravity = (0, -10.0 * gravity_multiplier)\n",
    "\n",
    "\n",
    "def make_env(task: str, render_mode=None):\n",
    "    \"\"\"Factory function to create environment variants.\n",
    "\n",
    "    Args:\n",
    "        task: One of 'standard', 'windy', 'heavy'\n",
    "        render_mode: Rendering mode (None for Colab)\n",
    "\n",
    "    Returns:\n",
    "        Environment instance\n",
    "    \"\"\"\n",
    "    task = task.lower()\n",
    "\n",
    "    if task == 'standard':\n",
    "        return StandardLunarLander(render_mode=render_mode)\n",
    "    elif task == 'windy':\n",
    "        return WindyLunarLander(render_mode=render_mode)\n",
    "    elif task == 'heavy':\n",
    "        return HeavyWeightLunarLander(render_mode=render_mode)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {task}. Choose from 'standard', 'windy', 'heavy'\")\n",
    "\n",
    "print(\"‚úì Environment variants defined (Standard, Windy, Heavy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Output Directory Setup\n# Create output directories (matching local structure)\noutput_base = '/content/results'\nos.makedirs(f'{output_base}/logs', exist_ok=True)\nos.makedirs(f'{output_base}/models', exist_ok=True)\nos.makedirs(f'{output_base}/plots', exist_ok=True)\n\nprint(f\"‚úì Output directories created at: {output_base}\")\nprint(f\"  - Logs: {output_base}/logs\")\nprint(f\"  - Models: {output_base}/models\")\nprint(f\"  - Plots: {output_base}/plots\")\nprint(\"\\nüìÅ Directory structure matches local setup!\")\nprint(\"   Download /content/results/ after training to merge with local results/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Helper Functions\ndef count_parameters(model):\n    \"\"\"Count total and trainable parameters in PyTorch model.\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return {'total': total_params, 'trainable': trainable_params}\n\n\ndef save_progress_checkpoint(rewards, losses, eval_rewards, eval_episodes, task_name):\n    \"\"\"Save training progress during training.\"\"\"\n    checkpoint_data = {\n        'episode_rewards': rewards,\n        'episode_losses': losses,\n        'eval_rewards': eval_rewards,\n        'eval_episodes': eval_episodes,\n        'last_episode': len(rewards),\n        'task': task_name,\n        'timestamp': time.strftime(\"%Y%m%d_%H%M%S\")\n    }\n    # Match baseline naming convention\n    with open(f'{output_base}/logs/independent_dqn_{task_name}_progress.json', 'w') as f:\n        json.dump(checkpoint_data, f, indent=2)\n\nprint(\"‚úì Helper functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Hyperparameters\nHYPERPARAMS = {\n    # Training\n    'num_episodes': 1000,\n    'batch_size': 64,\n    'replay_buffer_size': 100000,\n    'min_replay_size': 1000,\n\n    # DQN Agent\n    'learning_rate': 5e-4,\n    'gamma': 0.99,\n    'epsilon_start': 1.0,\n    'epsilon_end': 0.01,\n    'epsilon_decay': 0.995,\n    'target_update_freq': 10,\n\n    # Evaluation\n    'eval_freq': 50,\n    'eval_episodes': 5,\n\n    # Checkpointing\n    'save_freq': 100,\n\n    # Task - HEAVY WEIGHT VARIANT\n    'task': 'heavy',\n\n    # Device\n    'device': device\n}\n\nprint(\"Hyperparameters:\")\nprint(\"=\" * 60)\nfor key, value in HYPERPARAMS.items():\n    print(f\"  {key:<25} {value}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Environment & Agent Initialization\n",
    "# Create environment (render_mode=None for Colab)\n",
    "task_name = HYPERPARAMS['task']\n",
    "env = make_env(task_name, render_mode=None)\n",
    "\n",
    "# Get environment info\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment: {env.task_name} Lunar Lander\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=HYPERPARAMS['learning_rate'],\n",
    "    gamma=HYPERPARAMS['gamma'],\n",
    "    epsilon_start=HYPERPARAMS['epsilon_start'],\n",
    "    epsilon_end=HYPERPARAMS['epsilon_end'],\n",
    "    epsilon_decay=HYPERPARAMS['epsilon_decay'],\n",
    "    target_update_freq=HYPERPARAMS['target_update_freq'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=HYPERPARAMS['replay_buffer_size'])\n",
    "\n",
    "print(f\"\\n‚úì Created: {agent}\")\n",
    "print(f\"‚úì Created: {replay_buffer}\")\n",
    "\n",
    "# Count parameters\n",
    "q_network_params = count_parameters(agent.q_network)\n",
    "target_network_params = count_parameters(agent.target_network)\n",
    "total_model_params = q_network_params['total'] + target_network_params['total']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PARAMETER COUNTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Q-Network:      {q_network_params['total']:,} total, {q_network_params['trainable']:,} trainable\")\n",
    "print(f\"Target Network: {target_network_params['total']:,} total, {target_network_params['trainable']:,} trainable\")\n",
    "print(f\"Total:          {total_model_params:,} parameters\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "# Training statistics\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "eval_rewards = []\n",
    "eval_episodes = []\n",
    "\n",
    "# Sample Efficiency Metrics\n",
    "total_env_steps = 0\n",
    "total_gradient_updates = 0\n",
    "performance_thresholds = {50: None, 100: None, 150: None, 200: None}\n",
    "\n",
    "# Best model tracking\n",
    "best_eval_reward = -np.inf\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\nüöÄ Starting training on {task_name.upper()} task...\")\n",
    "print(\"üí° Progress bar will update below.\\n\")\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=HYPERPARAMS['num_episodes'],\n",
    "            desc=f\"Training {task_name}\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "\n",
    "# Training loop\n",
    "for episode in range(HYPERPARAMS['num_episodes']):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "    done = False\n",
    "    truncated = False\n",
    "    steps = 0\n",
    "\n",
    "    # Play one episode\n",
    "    while not (done or truncated):\n",
    "        # Select action\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Store transition\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Train if we have enough experiences\n",
    "        if len(replay_buffer) >= HYPERPARAMS['min_replay_size']:\n",
    "            batch = replay_buffer.sample(HYPERPARAMS['batch_size'])\n",
    "            loss = agent.update(batch)\n",
    "            episode_loss.append(loss)\n",
    "            total_gradient_updates += 1\n",
    "\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        total_env_steps += 1\n",
    "\n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "    agent.episodes += 1\n",
    "\n",
    "    # Update target network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Store statistics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "    episode_losses.append(avg_loss)\n",
    "    avg_reward_100 = np.mean(episode_rewards[-100:]) if episode_rewards else 0\n",
    "\n",
    "    # Check thresholds\n",
    "    for threshold, first_episode in performance_thresholds.items():\n",
    "        if first_episode is None and avg_reward_100 >= threshold:\n",
    "            performance_thresholds[threshold] = {\n",
    "                'episode': episode + 1,\n",
    "                'total_steps': total_env_steps,\n",
    "                'gradient_updates': total_gradient_updates\n",
    "            }\n",
    "            pbar.write(f\"üéØ Threshold {threshold} reached at episode {episode+1} (steps: {total_env_steps:,})\")\n",
    "\n",
    "    # Update progress bar\n",
    "    pbar.set_postfix({\n",
    "        'reward': f'{episode_reward:.1f}',\n",
    "        'avg_100': f'{avg_reward_100:.1f}',\n",
    "        'Œµ': f'{agent.epsilon:.3f}',\n",
    "        'loss': f'{avg_loss:.2f}'\n",
    "    })\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Print summary every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        pbar.write(f\"\\n[Episode {episode+1:4d}] Reward: {episode_reward:7.2f} | Avg(100): {avg_reward_100:7.2f} | Loss: {avg_loss:6.4f}\")\n",
    "        pbar.write(f\"               Steps: {total_env_steps:,} | Updates: {total_gradient_updates:,}\")\n",
    "\n",
    "    # Save progress checkpoint every 50 episodes\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        save_progress_checkpoint(episode_rewards, episode_losses, eval_rewards, eval_episodes, task_name)\n",
    "\n",
    "    # Evaluation\n",
    "    if episode % HYPERPARAMS['eval_freq'] == 0 and episode > 0:\n",
    "        eval_reward_mean = 0\n",
    "        eval_reward_list = []\n",
    "\n",
    "        for _ in range(HYPERPARAMS['eval_episodes']):\n",
    "            eval_state, _ = env.reset()\n",
    "            eval_reward = 0\n",
    "            eval_done = False\n",
    "            eval_truncated = False\n",
    "\n",
    "            while not (eval_done or eval_truncated):\n",
    "                eval_action = agent.select_action(eval_state, epsilon=0.0)\n",
    "                eval_state, r, eval_done, eval_truncated, _ = env.step(eval_action)\n",
    "                eval_reward += r\n",
    "\n",
    "            eval_reward_list.append(eval_reward)\n",
    "            eval_reward_mean += eval_reward\n",
    "\n",
    "        eval_reward_mean /= HYPERPARAMS['eval_episodes']\n",
    "        eval_reward_std = np.std(eval_reward_list)\n",
    "        eval_rewards.append(eval_reward_mean)\n",
    "        eval_episodes.append(episode)\n",
    "\n",
    "        pbar.write(\"-\" * 80)\n",
    "        if eval_reward_mean > best_eval_reward:\n",
    "            best_eval_reward = eval_reward_mean\n",
    "            model_path = f'{output_base}/models/independent_dqn_{task_name}.pth'\n",
    "            agent.save(model_path)\n",
    "            pbar.write(f\"[EVAL @ Episode {episode+1}] Mean: {eval_reward_mean:7.2f} (¬±{eval_reward_std:5.2f}) ‚≠ê NEW BEST!\")\n",
    "        else:\n",
    "            pbar.write(f\"[EVAL @ Episode {episode+1}] Mean: {eval_reward_mean:7.2f} (¬±{eval_reward_std:5.2f})\")\n",
    "        pbar.write(\"-\" * 80)\n",
    "\n",
    "    # Checkpoint\n",
    "    if episode % HYPERPARAMS['save_freq'] == 0 and episode > 0:\n",
    "        checkpoint_path = f'{output_base}/models/independent_dqn_{task_name}_ep{episode}.pth'\n",
    "        agent.save(checkpoint_path)\n",
    "        pbar.write(f\"[CHECKPOINT] Saved to {checkpoint_path}\")\n",
    "\n",
    "# Close progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Training complete\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Training on {task_name.upper()} complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training time: {training_time/60:.2f} minutes ({training_time:.1f} seconds)\")\n",
    "print(f\"Best eval reward: {best_eval_reward:.2f}\")\n",
    "print(f\"Final avg reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print()\n",
    "print(\"SAMPLE EFFICIENCY METRICS:\")\n",
    "print(f\"  Total environment steps: {total_env_steps:,}\")\n",
    "print(f\"  Total gradient updates: {total_gradient_updates:,}\")\n",
    "print(f\"  Steps per episode (avg): {total_env_steps / len(episode_rewards):.1f}\")\n",
    "print()\n",
    "print(\"PERFORMANCE THRESHOLDS REACHED:\")\n",
    "for threshold in sorted(performance_thresholds.keys()):\n",
    "    milestone = performance_thresholds[threshold]\n",
    "    if milestone:\n",
    "        print(f\"  Reward ‚â• {threshold:3d}: Episode {milestone['episode']:4d} | Steps: {milestone['total_steps']:,} | Updates: {milestone['gradient_updates']:,}\")\n",
    "    else:\n",
    "        print(f\"  Reward ‚â• {threshold:3d}: Not reached\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Metrics\n",
    "# Prepare metrics\n",
    "metrics = {\n",
    "    \"method\": \"Independent DQN\",\n",
    "    \"task\": task_name,\n",
    "    \"hyperparams\": HYPERPARAMS,\n",
    "    \"episode_rewards\": episode_rewards,\n",
    "    \"episode_losses\": episode_losses,\n",
    "    \"eval_rewards\": eval_rewards,\n",
    "    \"eval_episodes\": eval_episodes,\n",
    "    \"training_time\": training_time,\n",
    "    \"best_eval_reward\": best_eval_reward,\n",
    "\n",
    "    \"parameter_efficiency\": {\n",
    "        \"q_network_params\": q_network_params['total'],\n",
    "        \"target_network_params\": target_network_params['total'],\n",
    "        \"total_params\": total_model_params,\n",
    "        \"trainable_params\": q_network_params['trainable'],\n",
    "    },\n",
    "\n",
    "    \"sample_efficiency\": {\n",
    "        \"total_env_steps\": total_env_steps,\n",
    "        \"total_gradient_updates\": total_gradient_updates,\n",
    "        \"steps_per_episode_avg\": total_env_steps / len(episode_rewards),\n",
    "        \"performance_thresholds\": performance_thresholds,\n",
    "    },\n",
    "\n",
    "    \"conflict_robustness\": {\n",
    "        \"average_reward\": float(np.mean(episode_rewards)),\n",
    "        \"average_reward_last_100\": float(np.mean(episode_rewards[-100:])),\n",
    "        \"per_task_reward\": {\n",
    "            task_name: float(np.mean(episode_rewards[-100:]))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "json_path = f'{output_base}/logs/independent_dqn_{task_name}_metrics.json'\n",
    "pickle_path = f'{output_base}/logs/independent_dqn_{task_name}_metrics.pkl'\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úì Metrics saved successfully!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  JSON: {json_path}\")\n",
    "print(f\"  Pickle: {pickle_path}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Visualization\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "rewards = episode_rewards\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0, 0].plot(rewards, alpha=0.3, color='green', label='Raw')\n",
    "axes[0, 0].plot(range(len(smoothed)), smoothed, color='green', linewidth=2, label='Smoothed')\n",
    "axes[0, 0].axhline(200, color='red', linestyle='--', label='Success threshold')\n",
    "axes[0, 0].set_title(f'{task_name.capitalize()} - Episode Rewards', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "losses = episode_losses\n",
    "smoothed_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0, 1].plot(losses, alpha=0.3, color='red', label='Raw')\n",
    "axes[0, 1].plot(range(len(smoothed_loss)), smoothed_loss, color='red', linewidth=2, label='Smoothed')\n",
    "axes[0, 1].set_title('Training Loss', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Evaluation Rewards\n",
    "if eval_rewards:\n",
    "    axes[0, 2].plot(eval_episodes, eval_rewards, marker='o', color='blue', linewidth=2, markersize=6)\n",
    "    axes[0, 2].axhline(200, color='red', linestyle='--', label='Success')\n",
    "    axes[0, 2].axhline(best_eval_reward, color='gold', linestyle=':', linewidth=2, label=f'Best: {best_eval_reward:.1f}')\n",
    "    axes[0, 2].set_title('Evaluation Rewards', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Episode')\n",
    "    axes[0, 2].set_ylabel('Mean Reward')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Running Average\n",
    "running_avg = [np.mean(episode_rewards[max(0, i-99):i+1]) for i in range(len(episode_rewards))]\n",
    "axes[1, 0].plot(running_avg, color='darkgreen', linewidth=2)\n",
    "axes[1, 0].axhline(200, color='red', linestyle='--', label='Success')\n",
    "axes[1, 0].set_title('Running Average (100 episodes)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Average Reward')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Success Rate\n",
    "window_size = 50\n",
    "success_rate = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    start_idx = max(0, i - window_size + 1)\n",
    "    window_rewards = episode_rewards[start_idx:i+1]\n",
    "    rate = sum(r > 200 for r in window_rewards) / len(window_rewards) * 100\n",
    "    success_rate.append(rate)\n",
    "\n",
    "axes[1, 1].plot(success_rate, color='orange', linewidth=2)\n",
    "axes[1, 1].set_title(f'Success Rate (window={window_size})', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Success Rate (%)')\n",
    "axes[1, 1].set_ylim([0, 105])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Summary Stats\n",
    "final_avg = np.mean(episode_rewards[-100:])\n",
    "axes[1, 2].text(0.5, 0.7, f\"Final Performance\\n(Last 100 Episodes)\",\n",
    "                ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].text(0.5, 0.4, f\"{final_avg:.1f}\",\n",
    "                ha='center', va='center', fontsize=24, fontweight='bold',\n",
    "                color='green' if final_avg >= 200 else 'orange')\n",
    "axes[1, 2].text(0.5, 0.2, f\"Parameters: {total_model_params:,}\",\n",
    "                ha='center', va='center', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Independent DQN - {task_name.capitalize()} Task (Colab)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_base}/plots/independent_dqn_{task_name}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualization saved to: {output_base}/plots/independent_dqn_{task_name}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}